{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Crackup</h1>\n",
    "\n",
    "Our group goal was to pull out ATL03 and ATL06 segments from two areas where we saw crevasses, and compare small scale features in the two datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Location/time </h2>\n",
    "After a reeliminary look at the data, We pulled out a granules at two locations using KML files to and a short time    window\n",
    "\n",
    "<ul>\n",
    "    <li>georgeVI_meltpond-polygon.kml : Feb5 - Feb7, 2019\n",
    "    <li>edgeworth_simple-polygon.kml : Oct 14 - Oct 16, 2018\n",
    " </ul>\n",
    "\n",
    "\n",
    "We tried a more complex polygon, and this program had trouble finding the data.  So we had to go with a simple polygon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Downloading ATL03 and ATL06 data</h2>\n",
    "The code we used to get the ATL03 and ATLO6 data for a specific time period and within a KML polygon is shown below.  It is broken up into two sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Earthdata Login password:  ·········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D99C2A43-6B49-246B-FCEE-264187F33E09\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "import getpass\n",
    "import socket\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import pprint\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import fiona\n",
    "import h5py\n",
    "import re\n",
    "# To read KML files with geopandas, we will need to enable KML support in fiona (disabled by default)\n",
    "fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.geometry.polygon import orient\n",
    "from statistics import mean\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# Earthdata Login credentials\n",
    "\n",
    "# Enter your Earthdata Login user name\n",
    "uid = 'slhoward'\n",
    "# Enter your email address associated with your Earthdata Login account\n",
    "email = 'showard@esr.org'\n",
    "pswd = getpass.getpass('Earthdata Login password: ')\n",
    "\n",
    "# Request token from Common Metadata Repository using Earthdata credentials\n",
    "token_api_url = 'https://cmr.earthdata.nasa.gov/legacy-services/rest/tokens'\n",
    "hostname = socket.gethostname()\n",
    "ip = socket.gethostbyname(hostname)\n",
    "\n",
    "data = {\n",
    "    'token': {\n",
    "        'username': uid,\n",
    "        'password': pswd,\n",
    "        'client_id': 'NSIDC_client_id',\n",
    "        'user_ip_address': ip\n",
    "    }\n",
    "}\n",
    "headers={'Accept': 'application/json'}\n",
    "response = requests.post(token_api_url, json=data, headers=headers)\n",
    "token = json.loads(response.content)['token']['id']\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In the code below, set:\n",
    "<ul> \n",
    "    <li>short_name: ATL03 or ATL06 \n",
    "    <li>temporal range\n",
    "    <li>kml_filepath\n",
    "    <li> output file directory.  Default is ./Output_files\n",
    "</ul>\n",
    "</b>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001\n",
      "2019-02-05T00:00:00Z,2019-02-07T23:59:59Z\n",
      "/home/jovyan/crackup/shapefiles/georgeVI_meltpond-polygon.kml\n",
      "https://n5eil02u.ecs.nsidc.org/egi/capabilities/ATL03.001.xml\n",
      "[{'id': 'ICESAT2', 'spatialSubsetting': 'true', 'spatialSubsettingShapefile': 'true', 'temporalSubsetting': 'true', 'type': 'both', 'maxGransSyncRequest': '100', 'maxGransAsyncRequest': '2000'}]\n",
      "2019-02-05T00:00:00,2019-02-07T23:59:59\n",
      "1\n",
      "Order:  1\n",
      "Request HTTP response:  201\n",
      "order ID:  5000000320372\n",
      "status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000000320372\n",
      "HTTP response from order response URL:  201\n",
      "Data request  1  is submitting...\n",
      "Initial request status is  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  complete\n",
      "Zip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000000320372.zip\n",
      "Beginning download of zipped output...\n",
      "Data request 1 is complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['processed_ATL03_20190206021109_06050210_001_01.h5',\n",
       " 'processed_ATL03_20190206145852_06130212_001_01.h5']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input data set ID (e.g. ATL06) of interest here, also known as \"short name\".\n",
    "\n",
    "short_name = 'ATL03'\n",
    "\n",
    "# Get json response from CMR collection metadata and print results. This provides high-level metadata on a data set or \"collection\", provide in json format.\n",
    "\n",
    "params = {\n",
    "    'short_name': short_name\n",
    "}\n",
    "\n",
    "cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "response = requests.get(cmr_collections_url, params=params)\n",
    "results = json.loads(response.content)\n",
    "#pprint.pprint(results)\n",
    "\n",
    "# Find all instances of 'version_id' in metadata and print most recent version number\n",
    "\n",
    "versions = [i['version_id'] for i in results['feed']['entry']]\n",
    "latest_version = max(versions)\n",
    "print(latest_version)\n",
    "\n",
    "# Input temporal range \n",
    "\n",
    "# Input start date in yyyy-MM-dd format\n",
    "start_date = '2019-02-05'\n",
    "# Input start time in HH:mm:ss format\n",
    "start_time = '00:00:00'\n",
    "# Input end date in yyyy-MM-dd format\n",
    "end_date = '2019-02-07'\n",
    "# Input end time in HH:mm:ss format\n",
    "end_time = '23:59:59'\n",
    "\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'\n",
    "print(temporal)\n",
    "\n",
    "# Use geopandas to read in polygon file\n",
    "# Note: a shapefile or geojson, or almost any other vector-based spatial data format could be substituted here.\n",
    "\n",
    "kml_filepath = str('/home/jovyan/crackup/shapefiles/georgeVI_meltpond-polygon.kml')\n",
    "print(kml_filepath)\n",
    "#Return a GeoDataFrame object\n",
    "gdf = gpd.read_file(kml_filepath)\n",
    "\n",
    "#Integer position based indexing of GeoDataFrame object to get it into a shapeply geometry object.\n",
    "poly = gdf.iloc[0].geometry\n",
    "\n",
    "# Simplify polygon. The larger the tolerance value, the more simplified the polygon.\n",
    "poly = poly.simplify(0.05, preserve_topology=False)\n",
    "\n",
    "# Orient counter-clockwise\n",
    "poly = orient(poly, sign=1.0)\n",
    "\n",
    "#print(poly)\n",
    "\n",
    "#Format dictionary to polygon coordinate pairs for CMR polygon filtering\n",
    "polygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\n",
    "\n",
    "# aoi value used for CMR params below\n",
    "aoi = '3'\n",
    "\n",
    "\n",
    "#Create CMR parameters used for granule search. Modify params depending on bounding_box or polygon input.\n",
    "\n",
    "if aoi == '1':\n",
    "# bounding box input:\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'bounding_box': bounding_box\n",
    "    }\n",
    "else:\n",
    "    \n",
    "# If polygon input (either via coordinate pairs or shapefile/KML/KMZ):\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'polygon': polygon,\n",
    "    }\n",
    "\n",
    "#print('CMR search parameters: ', params)\n",
    "\n",
    "# Query number of granules using our (paging over results)\n",
    "\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "\n",
    "granules = []\n",
    "while True:\n",
    "    response = requests.get(granule_search_url, params=params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    params['page_num'] += 1\n",
    "\n",
    "    \n",
    "# Get number of granules over my area and time of interest\n",
    "len(granules)\n",
    "\n",
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "\n",
    "# Average size of granules in MB\n",
    "mean(granule_sizes)\n",
    "\n",
    "# Total volume in MB\n",
    "sum(granule_sizes)\n",
    "\n",
    "# Query service capability URL \n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "\n",
    "print(capability_url)\n",
    "\n",
    "# Create session to store cookie and pass credentials to capabilities url\n",
    "\n",
    "session = requests.session()\n",
    "s = session.get(capability_url)\n",
    "response = session.get(s.url,auth=(uid,pswd))\n",
    "\n",
    "root = ET.fromstring(response.content)\n",
    "\n",
    "# collect lists with each service option\n",
    "\n",
    "subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "\n",
    "# variable subsetting\n",
    "variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "\n",
    "# reformatting\n",
    "formats = [Format.attrib for Format in root.iter('Format')]\n",
    "format_vals = [formats[i]['value'] for i in range(len(formats))]\n",
    "format_vals.remove('')\n",
    "\n",
    "# reprojection only applicable on ICESat-2 L3B products, yet to be available. \n",
    "\n",
    "# reformatting options that support reprojection\n",
    "normalproj = [Projections.attrib for Projections in root.iter('Projections')]\n",
    "normalproj_vals = []\n",
    "normalproj_vals.append(normalproj[0]['normalProj'])\n",
    "format_proj = normalproj_vals[0].split(',')\n",
    "format_proj.remove('')\n",
    "format_proj.append('No reformatting')\n",
    "\n",
    "#reprojection options\n",
    "projections = [Projection.attrib for Projection in root.iter('Projection')]\n",
    "proj_vals = []\n",
    "for i in range(len(projections)):\n",
    "    if (projections[i]['value']) != 'NO_CHANGE' :\n",
    "        proj_vals.append(projections[i]['value'])\n",
    "        \n",
    "# reformatting options that do not support reprojection\n",
    "no_proj = [i for i in format_vals if i not in format_proj]\n",
    "\n",
    "print(subagent)\n",
    "if len(subagent) < 1 :\n",
    "    agent = 'NO'\n",
    "    \n",
    "# Temporal subsetting KVP\n",
    "\n",
    "timevar = start_date + 'T' + start_time + ',' + end_date + 'T' + end_time\n",
    "print(timevar) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Set NSIDC data access base URL\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "\n",
    "# Set number of granules requested per order, which we will initially set to 10.\n",
    "page_size = 10\n",
    "\n",
    "#Determine number of pages basd on page_size and total granules. Loop requests by this value\n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "\n",
    "#Set request mode. \n",
    "request_mode = 'async'\n",
    "\n",
    "# Determine how many individual orders we will request based on the number of granules requested\n",
    "\n",
    "print(page_num)\n",
    "\n",
    "API_request = f'{base_url}?short_name={short_name}&version={latest_version}&temporal={temporal}&time={timevar}&polygon={polygon}&Coverage={coverage}&request_mode={request_mode}&page_size={page_size}&page_num={page_num}&token={token}&email={email}'\n",
    "\n",
    "\n",
    "subset_params = {\n",
    "    'short_name': short_name, \n",
    "    'version': latest_version, \n",
    "    'temporal': temporal, \n",
    "    'time': timevar, \n",
    "    'polygon': polygon, \n",
    "#    'Coverage': coverage, \n",
    "    'request_mode': request_mode, \n",
    "    'page_size': page_size,  \n",
    "    'token': token, \n",
    "    'email': email, \n",
    "    }\n",
    "#print(subset_params)\n",
    "\n",
    "path = str(os.getcwd() + '/Output_files')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    \n",
    "# Request data service for each page number, and unzip outputs\n",
    "\n",
    "for i in range(page_num):\n",
    "    page_val = i + 1\n",
    "    print('Order: ', page_val)\n",
    "    subset_params.update( {'page_num': page_val} )\n",
    "    \n",
    "# Post polygon to API endpoint for polygon subsetting to subset based on original, non-simplified KML file\n",
    "\n",
    "    shape_post = {'shapefile': open(kml_filepath, 'rb')}\n",
    "    request = session.post(base_url, params=subset_params, files=shape_post) \n",
    "    \n",
    "# FOR ALL OTHER REQUESTS THAT DO NOT UTILIZED AN UPLOADED POLYGON FILE, USE A GET REQUEST INSTEAD OF POST:\n",
    "#     request = session.get(base_url, params=request_params)\n",
    "    \n",
    "    print('Request HTTP response: ', request.status_code)\n",
    "\n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "    request.raise_for_status()\n",
    "#    print('Order request URL: ', request.url)\n",
    "    esir_root = ET.fromstring(request.content)\n",
    "#    print('Order request response XML content: ', request.content)\n",
    "\n",
    "# Look up order ID\n",
    "    orderlist = []   \n",
    "    for order in esir_root.findall(\"./order/\"):\n",
    "        orderlist.append(order.text)\n",
    "    orderID = orderlist[0]\n",
    "    print('order ID: ', orderID)\n",
    "\n",
    "# Create status URL\n",
    "    statusURL = base_url + '/' + orderID\n",
    "    print('status URL: ', statusURL)\n",
    "\n",
    "# Find order status\n",
    "    request_response = session.get(statusURL)    \n",
    "    print('HTTP response from order response URL: ', request_response.status_code)\n",
    "    \n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "    request_response.raise_for_status()\n",
    "    request_root = ET.fromstring(request_response.content)\n",
    "    statuslist = []\n",
    "    for status in request_root.findall(\"./requestStatus/\"):\n",
    "        statuslist.append(status.text)\n",
    "    status = statuslist[0]\n",
    "    print('Data request ', page_val, ' is submitting...')\n",
    "    print('Initial request status is ', status)\n",
    "\n",
    "# Continue to loop while request is still processing\n",
    "    while status == 'pending' or status == 'processing': \n",
    "        print('Status is not complete. Trying again.')\n",
    "        time.sleep(10)\n",
    "        loop_response = session.get(statusURL)\n",
    "\n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "        loop_response.raise_for_status()\n",
    "        loop_root = ET.fromstring(loop_response.content)\n",
    "\n",
    "# Find status\n",
    "        statuslist = []\n",
    "        for status in loop_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Retry request status is: ', status)\n",
    "        if status == 'pending' or status == 'processing':\n",
    "            continue\n",
    "\n",
    "# Order can either complete, complete_with_errors, or fail:\n",
    "# Provide complete_with_errors error message:\n",
    "    if status == 'complete_with_errors' or status == 'failed':\n",
    "        messagelist = []\n",
    "        for message in loop_root.findall(\"./processInfo/\"):\n",
    "            messagelist.append(message.text)\n",
    "        print('error messages:')\n",
    "        pprint.pprint(messagelist)\n",
    "\n",
    "# Download zipped order if status is complete or complete_with_errors\n",
    "    if status == 'complete' or status == 'complete_with_errors':\n",
    "        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "        print('Zip download URL: ', downloadURL)\n",
    "        print('Beginning download of zipped output...')\n",
    "        zip_response = session.get(downloadURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        zip_response.raise_for_status()\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "            z.extractall(path)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "    else: print('Request failed.')\n",
    "\n",
    "#Clean up Outputs folder by removing individual granule folders \n",
    "\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        try:\n",
    "            shutil.move(os.path.join(root, file), path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        \n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        os.rmdir(os.path.join(root, name))\n",
    "        \n",
    "#List files\n",
    "sorted(os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
