{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Crackup</h1>\n",
    "<p><i> ICESAT-2 hackweek, June 17-21, 2019</i></p>\n",
    "\n",
    "\n",
    "<p>Our group goal was to pull out ATL03 and ATL06 segments from two areas where we saw crevasses, and compare small scale features in the two datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Location/time </h2>\n",
    "After a preliminary look at the data, we pulled out a granules at two locations using KML files and a short time    window\n",
    "\n",
    "<ul>\n",
    "    <li>georgeVI_meltpond-polygon.kml : Feb5 - Feb7, 2019\n",
    "    <li>edgeworth_simple-polygon.kml : Oct 14 - Oct 16, 2018\n",
    " </ul>\n",
    "\n",
    "\n",
    "We tried a more complex polygon, and this program had trouble finding the data.  So we had to go with a simple polygon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Downloading ATL03 and ATL06 data</h2>\n",
    "The code we used to get the ATL03 and ATLO6 data for a specific time period and within a KML polygon is shown below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In the code below, set:\n",
    "<ul> \n",
    "    <li>short_name: ATL03 or ATL06 \n",
    "    <li>temporal range\n",
    "    <li>kml_filepath\n",
    "    <li> output file directory.  Default is ./Output_files\n",
    "</ul>\n",
    "</b>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Libaries for download script</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import libraries\n",
    "import requests\n",
    "import getpass\n",
    "import socket\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "import math\n",
    "import os,csv\n",
    "import shutil\n",
    "import pprint\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import fiona\n",
    "import h5py\n",
    "import re\n",
    "import glob\n",
    "# To read KML files with geopandas, we will need to enable KML support in fiona (disabled by default)\n",
    "fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.geometry.polygon import orient\n",
    "from statistics import mean\n",
    "from requests.auth import HTTPBasicAuth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>In the code below, set:\n",
    "<ul> \n",
    "    <li>short_name: ATL03 or ATL06 \n",
    "    <li>temporal range\n",
    "    <li>kml_filepath\n",
    "    <li> output file directory \n",
    "    <li> Earthdata login credentials\n",
    "</ul>\n",
    "</b>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/crackup/Edgeworth/\n",
      "/home/jovyan/crackup/shapefiles/edgeworth_simple-polygon.kml\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Earthdata Login password:  ············\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2BCA6BFA-40D5-D958-F55A-C3BB3C90910D\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input data set ID (e.g. ATL06) of interest here, also known as \"short name\".\n",
    "\n",
    "#read the csv file necessary for looping\n",
    "with open('region.csv') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    region = {row[0]:[row[1].strip(),row[2].strip()] for row in rows}   # region name : [datadir, shape filename]\n",
    "    dival = {'dataDir':0,'shapefile':1}\n",
    "\n",
    "reg = 'Edgeworth'\n",
    "dataDir = region[reg][dival['dataDir']]\n",
    "shapef = region[reg][dival['shapefile']]\n",
    "print(dataDir)\n",
    "print(shapef)\n",
    "\n",
    "#specify the dataset\n",
    "short_name = 'ATL03'\n",
    "\n",
    "# Input temporal range \n",
    "\n",
    "# Input start date in yyyy-MM-dd format\n",
    "start_date = '2018-10-14'\n",
    "# Input start time in HH:mm:ss format\n",
    "start_time = '00:00:00'\n",
    "# Input end date in yyyy-MM-dd format\n",
    "end_date = '2018-10-16'\n",
    "# Input end time in HH:mm:ss format\n",
    "end_time = '23:59:59'\n",
    "\n",
    "\n",
    "#shapefile\n",
    "kml_filepath=shapef\n",
    "#kml_filepath = str('/home/jovyan/crackup/shapefiles/georgeVI_meltpond-polygon.kml')\n",
    "# kml_filepath = str('/home/jovyan/crackup/shapefiles/edgeworth_simple-polygon.kml')\n",
    "\n",
    "# set directory for subsetted files\n",
    "path = dataDir\n",
    "# path = str(os.getcwd() + '/ATL_Output')\n",
    "\n",
    "# Earthdata Login credentials\n",
    "\n",
    "# Enter your Earthdata Login user name\n",
    "uid = 'ellyn.enderlin'\n",
    "# Enter your email address associated with your Earthdata Login account\n",
    "email = 'ellynenderlin@boisestate.edu'\n",
    "pswd = getpass.getpass('Earthdata Login password: ')\n",
    "\n",
    "# Request token from Common Metadata Repository using Earthdata credentials\n",
    "token_api_url = 'https://cmr.earthdata.nasa.gov/legacy-services/rest/tokens'\n",
    "hostname = socket.gethostname()\n",
    "ip = socket.gethostbyname(hostname)\n",
    "\n",
    "data = {\n",
    "    'token': {\n",
    "        'username': uid,\n",
    "        'password': pswd,\n",
    "        'client_id': 'NSIDC_client_id',\n",
    "        'user_ip_address': ip\n",
    "    }\n",
    "}\n",
    "headers={'Accept': 'application/json'}\n",
    "response = requests.post(token_api_url, json=data, headers=headers)\n",
    "token = json.loads(response.content)['token']['id']\n",
    "print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001\n",
      "/home/jovyan/crackup/shapefiles/edgeworth_simple-polygon.kml\n",
      "2018-10-14T00:00:00Z,2018-10-16T23:59:59Z\n",
      "https://n5eil02u.ecs.nsidc.org/egi/capabilities/ATL03.001.xml\n",
      "[{'id': 'ICESAT2', 'spatialSubsetting': 'true', 'spatialSubsettingShapefile': 'true', 'temporalSubsetting': 'true', 'type': 'both', 'maxGransSyncRequest': '100', 'maxGransAsyncRequest': '2000'}]\n",
      "2018-10-14T00:00:00,2018-10-16T23:59:59\n",
      "1\n",
      "Order:  1\n",
      "Request HTTP response:  201\n",
      "order ID:  5000000320515\n",
      "status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000000320515\n",
      "HTTP response from order response URL:  201\n",
      "Data request  1  is submitting...\n",
      "Initial request status is  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  complete\n",
      "Zip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000000320515.zip\n",
      "Beginning download of zipped output...\n",
      "Data request 1 is complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3plot_ATL03_ALT06_Edgeworth-checkpoint.ipynb',\n",
       " '3plot_ATL03_ALT06_Edgeworth.ipynb',\n",
       " 'Edgeworth_ATL03_20181015194309_02620112_001_01.h5',\n",
       " 'Edgeworth_ATL06_20181015194309_02620112_001_01.h5',\n",
       " 'processed_ATL03_20181015194309_02620112_001_01.h5']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data:\n",
    "\n",
    "# Get json response from CMR collection metadata and print results. This provides high-level metadata on a data set or \"collection\", provide in json format.\n",
    "\n",
    "params = {\n",
    "    'short_name': short_name\n",
    "}\n",
    "\n",
    "cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "response = requests.get(cmr_collections_url, params=params)\n",
    "results = json.loads(response.content)\n",
    "#pprint.pprint(results)\n",
    "\n",
    "# Find all instances of 'version_id' in metadata and print most recent version number\n",
    "\n",
    "versions = [i['version_id'] for i in results['feed']['entry']]\n",
    "latest_version = max(versions)\n",
    "print(latest_version)\n",
    "\n",
    "print(kml_filepath)\n",
    "\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'\n",
    "print(temporal)\n",
    "\n",
    "# Use geopandas to read in polygon file\n",
    "# Note: a shapefile or geojson, or almost any other vector-based spatial data format could be substituted here.\n",
    "\n",
    "\n",
    "#Return a GeoDataFrame object\n",
    "gdf = gpd.read_file(kml_filepath)\n",
    "\n",
    "#Integer position based indexing of GeoDataFrame object to get it into a shapeply geometry object.\n",
    "poly = gdf.iloc[0].geometry\n",
    "\n",
    "# Simplify polygon. The larger the tolerance value, the more simplified the polygon.\n",
    "poly = poly.simplify(0.05, preserve_topology=False)\n",
    "\n",
    "# Orient counter-clockwise\n",
    "poly = orient(poly, sign=1.0)\n",
    "\n",
    "#print(poly)\n",
    "\n",
    "#Format dictionary to polygon coordinate pairs for CMR polygon filtering\n",
    "polygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\n",
    "\n",
    "# aoi value used for CMR params below\n",
    "aoi = '3'\n",
    "\n",
    "\n",
    "#Create CMR parameters used for granule search. Modify params depending on bounding_box or polygon input.\n",
    "\n",
    "if aoi == '1':\n",
    "# bounding box input:\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'bounding_box': bounding_box\n",
    "    }\n",
    "else:\n",
    "    \n",
    "# If polygon input (either via coordinate pairs or shapefile/KML/KMZ):\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'polygon': polygon,\n",
    "    }\n",
    "\n",
    "#print('CMR search parameters: ', params)\n",
    "\n",
    "# Query number of granules using our (paging over results)\n",
    "\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "\n",
    "granules = []\n",
    "while True:\n",
    "    response = requests.get(granule_search_url, params=params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    params['page_num'] += 1\n",
    "\n",
    "    \n",
    "# Get number of granules over my area and time of interest\n",
    "len(granules)\n",
    "\n",
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "\n",
    "# Average size of granules in MB\n",
    "mean(granule_sizes)\n",
    "\n",
    "# Total volume in MB\n",
    "sum(granule_sizes)\n",
    "\n",
    "# Query service capability URL \n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "\n",
    "print(capability_url)\n",
    "\n",
    "# Create session to store cookie and pass credentials to capabilities url\n",
    "\n",
    "session = requests.session()\n",
    "s = session.get(capability_url)\n",
    "response = session.get(s.url,auth=(uid,pswd))\n",
    "\n",
    "root = ET.fromstring(response.content)\n",
    "\n",
    "# collect lists with each service option\n",
    "\n",
    "subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "\n",
    "# variable subsetting\n",
    "variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "\n",
    "# reformatting\n",
    "formats = [Format.attrib for Format in root.iter('Format')]\n",
    "format_vals = [formats[i]['value'] for i in range(len(formats))]\n",
    "format_vals.remove('')\n",
    "\n",
    "# reprojection only applicable on ICESat-2 L3B products, yet to be available. \n",
    "\n",
    "# reformatting options that support reprojection\n",
    "normalproj = [Projections.attrib for Projections in root.iter('Projections')]\n",
    "normalproj_vals = []\n",
    "normalproj_vals.append(normalproj[0]['normalProj'])\n",
    "format_proj = normalproj_vals[0].split(',')\n",
    "format_proj.remove('')\n",
    "format_proj.append('No reformatting')\n",
    "\n",
    "#reprojection options\n",
    "projections = [Projection.attrib for Projection in root.iter('Projection')]\n",
    "proj_vals = []\n",
    "for i in range(len(projections)):\n",
    "    if (projections[i]['value']) != 'NO_CHANGE' :\n",
    "        proj_vals.append(projections[i]['value'])\n",
    "        \n",
    "# reformatting options that do not support reprojection\n",
    "no_proj = [i for i in format_vals if i not in format_proj]\n",
    "\n",
    "print(subagent)\n",
    "if len(subagent) < 1 :\n",
    "    agent = 'NO'\n",
    "    \n",
    "# Temporal subsetting KVP\n",
    "\n",
    "timevar = start_date + 'T' + start_time + ',' + end_date + 'T' + end_time\n",
    "print(timevar) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Set NSIDC data access base URL\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "\n",
    "# Set number of granules requested per order, which we will initially set to 10.\n",
    "page_size = 10\n",
    "\n",
    "#Determine number of pages basd on page_size and total granules. Loop requests by this value\n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "\n",
    "#Set request mode. \n",
    "request_mode = 'async'\n",
    "\n",
    "# Determine how many individual orders we will request based on the number of granules requested\n",
    "\n",
    "print(page_num)\n",
    "\n",
    "\n",
    "subset_params = {\n",
    "    'short_name': short_name, \n",
    "    'version': latest_version, \n",
    "    'temporal': temporal, \n",
    "    'time': timevar, \n",
    "    'polygon': polygon, \n",
    "#    'Coverage': coverage, \n",
    "    'request_mode': request_mode, \n",
    "    'page_size': page_size,  \n",
    "    'token': token, \n",
    "    'email': email, \n",
    "    }\n",
    "#print(subset_params)\n",
    "\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    \n",
    "# Request data service for each page number, and unzip outputs\n",
    "\n",
    "for i in range(page_num):\n",
    "    page_val = i + 1\n",
    "    print('Order: ', page_val)\n",
    "    subset_params.update( {'page_num': page_val} )\n",
    "    \n",
    "# Post polygon to API endpoint for polygon subsetting to subset based on original, non-simplified KML file\n",
    "\n",
    "    shape_post = {'shapefile': open(kml_filepath, 'rb')}\n",
    "    request = session.post(base_url, params=subset_params, files=shape_post) \n",
    "    \n",
    "# FOR ALL OTHER REQUESTS THAT DO NOT UTILIZED AN UPLOADED POLYGON FILE, USE A GET REQUEST INSTEAD OF POST:\n",
    "#     request = session.get(base_url, params=request_params)\n",
    "    \n",
    "    print('Request HTTP response: ', request.status_code)\n",
    "\n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "    request.raise_for_status()\n",
    "#    print('Order request URL: ', request.url)\n",
    "    esir_root = ET.fromstring(request.content)\n",
    "#    print('Order request response XML content: ', request.content)\n",
    "\n",
    "# Look up order ID\n",
    "    orderlist = []   \n",
    "    for order in esir_root.findall(\"./order/\"):\n",
    "        orderlist.append(order.text)\n",
    "    orderID = orderlist[0]\n",
    "    print('order ID: ', orderID)\n",
    "\n",
    "# Create status URL\n",
    "    statusURL = base_url + '/' + orderID\n",
    "    print('status URL: ', statusURL)\n",
    "\n",
    "# Find order status\n",
    "    request_response = session.get(statusURL)    \n",
    "    print('HTTP response from order response URL: ', request_response.status_code)\n",
    "    \n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "    request_response.raise_for_status()\n",
    "    request_root = ET.fromstring(request_response.content)\n",
    "    statuslist = []\n",
    "    for status in request_root.findall(\"./requestStatus/\"):\n",
    "        statuslist.append(status.text)\n",
    "    status = statuslist[0]\n",
    "    print('Data request ', page_val, ' is submitting...')\n",
    "    print('Initial request status is ', status)\n",
    "\n",
    "# Continue to loop while request is still processing\n",
    "    while status == 'pending' or status == 'processing': \n",
    "        print('Status is not complete. Trying again.')\n",
    "        time.sleep(10)\n",
    "        loop_response = session.get(statusURL)\n",
    "\n",
    "# Raise bad request: Loop will stop for bad response code.\n",
    "        loop_response.raise_for_status()\n",
    "        loop_root = ET.fromstring(loop_response.content)\n",
    "\n",
    "# Find status\n",
    "        statuslist = []\n",
    "        for status in loop_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Retry request status is: ', status)\n",
    "        if status == 'pending' or status == 'processing':\n",
    "            continue\n",
    "\n",
    "# Order can either complete, complete_with_errors, or fail:\n",
    "# Provide complete_with_errors error message:\n",
    "    if status == 'complete_with_errors' or status == 'failed':\n",
    "        messagelist = []\n",
    "        for message in loop_root.findall(\"./processInfo/\"):\n",
    "            messagelist.append(message.text)\n",
    "        print('error messages:')\n",
    "        pprint.pprint(messagelist)\n",
    "\n",
    "# Download zipped order if status is complete or complete_with_errors\n",
    "    if status == 'complete' or status == 'complete_with_errors':\n",
    "        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "        print('Zip download URL: ', downloadURL)\n",
    "        print('Beginning download of zipped output...')\n",
    "        zip_response = session.get(downloadURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        zip_response.raise_for_status()\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "            z.extractall(path)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "    else: print('Request failed.')\n",
    "\n",
    "#Clean up Outputs folder by removing individual granule folders \n",
    "\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        try:\n",
    "            shutil.move(os.path.join(root, file), path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        \n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        os.rmdir(os.path.join(root, name))\n",
    "        \n",
    "#List files\n",
    "sorted(os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Plotting routine to make simple plots showing location of ATL03 granules</h3>\n",
    "    * Select file, file path, and beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Magic function to enable interactive plotting in Jupyter notebook\n",
    "#Allows you to zoom/pan within plots after generating\n",
    "#Normally, this would be %matplotlib notebook, but since we're using Juptyerlab, we need a different widget\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Import necesary modules\n",
    "#Use shorter names (np, pd, plt) instead of full (numpy, pandas, matplotlib.pylot) for convenience\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shapereader\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import s3fs\n",
    "import readers as rd\n",
    "import readers.utils as ut\n",
    "from glob import glob\n",
    "# Use seaborn for nicer looking inline plots\n",
    "#sns.set(context='notebook', style='darkgrid')\n",
    "#st = axes_style(\"whitegrid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected files to plot\n",
    "localFilePath=glob(dataDir + '*ATL03*.h5')[0]\n",
    "# ATL06file=glob(dataDir + '*ATL06*.h5')\n",
    "\n",
    "# ATL03file='Edgeworth_ATL03_20190206145852_06130212_001_01.h5'\n",
    "# localFilePath='/home/jovyan/crackup/Edgeworth/'+ATL03file\n",
    "\n",
    "# ATL03file2='processed_ATL03_20181015194309_02620112_001_01.h5'\n",
    "# localFilePath2='/home/jovyan/crackup/Outputs_selected/edgeworth/'+ATL03file2\n",
    "\n",
    "#select beam\n",
    "beamStr='gt1r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getATL03data(fileT, numpyout=False, beam='gt1r'):\n",
    "    \"\"\" Pandas/numpy ATL03 reader\n",
    "    Written by Alek Petty, June 2018 (alek.a.petty@nasa.gov)\n",
    "\n",
    "    I've picked out the variables from ATL03 I think are of most interest to sea ice users, but by no\n",
    "    means is this an exhastive list. \n",
    "    See the xarray or dictionary readers to load in the more complete ATL03 dataset\n",
    "    or explore the hdf5 files themselves (I like using the app Panpoly for this) to see what else you\n",
    "    might want\n",
    "    \n",
    "    Args:\n",
    "        fileT (str): File path of the ATL03 dataset\n",
    "        numpy (flag): Binary flag for outputting numpy arrays (True) or pandas dataframe (False)\n",
    "        beam (str): ICESat-2 beam (the number is the pair, r=strong, l=weak)\n",
    "        \n",
    "    returns:\n",
    "        either: select numpy arrays or a pandas dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the file\n",
    "    try:\n",
    "        ATL03 = h5py.File(fileT, 'r')\n",
    "    except:\n",
    "        'Not a valid file'\n",
    "        \n",
    "    lons=ATL03[beam+'/heights/lon_ph'][:]\n",
    "    lats=ATL03[beam+'/heights/lat_ph'][:]\n",
    "    \n",
    "    #  Number of seconds since the GPS epoch on midnight Jan. 6, 1980 \n",
    "    delta_time=ATL03[beam+'/heights/delta_time'][:] \n",
    "    \n",
    "    # #Add this value to delta time parameters to compute the full gps_seconds\n",
    "    atlas_epoch=ATL03['/ancillary_data/atlas_sdp_gps_epoch'][:] \n",
    "    \n",
    "    # Conversion of delta_time to a calendar date\n",
    "    # This function seems pretty convoluted but it works for now..\n",
    "    # Sure there is a simpler functionw e can use here instead.\n",
    "    temp = ut.convert_GPS_time(atlas_epoch[0] + delta_time, OFFSET=0.0)\n",
    "    \n",
    "    # Express delta_time relative to start time of granule\n",
    "    delta_time_granule=delta_time-delta_time[0]\n",
    "    #delta_time_granule=delta_time\n",
    "    year = temp['year'][:].astype('int')\n",
    "    month = temp['month'][:].astype('int')\n",
    "    day = temp['day'][:].astype('int')\n",
    "    hour = temp['hour'][:].astype('int')\n",
    "    minute = temp['minute'][:].astype('int')\n",
    "    second = temp['second'][:].astype('int')\n",
    "    \n",
    "    dFtime=pd.DataFrame({'year':year, 'month':month, 'day':day, \n",
    "                        'hour':hour, 'minute':minute, 'second':second})\n",
    "    \n",
    "    \n",
    "    # Primary variables of interest\n",
    "    \n",
    "    # Photon height\n",
    "    heights=ATL03[beam+'/heights/h_ph'][:]\n",
    "    #print(heights.shape)\n",
    "    \n",
    "    # Flag for signal confidence\n",
    "    # column index:  0=Land; 1=Ocean; 2=SeaIce; 3=LandIce; 4=InlandWater\n",
    "    # values:\n",
    "        #-- -1: Events not associated with a specific surface type\n",
    "        #--  0: noise\n",
    "        #--  1: buffer but algorithm classifies as background\n",
    "        #--  2: low\n",
    "        #--  3: medium\n",
    "        #--  4: high\n",
    "    signal_confidence=ATL03[beam+'/heights/signal_conf_ph'][:,3] \n",
    "    \n",
    "    # Add photon rate, background rate etc to the reader here if we want\n",
    "    \n",
    "    ATL03.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    dF = pd.DataFrame({'heights':heights, 'lons':lons, 'lats':lats,\n",
    "                       'signal_confidence':signal_confidence, \n",
    "                       'delta_time':delta_time_granule})\n",
    "    \n",
    "    # Add the datetime string\n",
    "    dFtimepd=pd.to_datetime(dFtime)\n",
    "    dF['datetime'] = pd.Series(dFtimepd, index=dF.index)\n",
    "    \n",
    "    # Filter out high elevation values \n",
    "    #dF = dF[(dF['signal_confidence']>2)]\n",
    "    # Reset row indexing\n",
    "    #dF=dF.reset_index(drop=True)\n",
    "    return dF\n",
    "    \n",
    "    # Or return as numpy arrays \n",
    "    # return along_track_distance, heights\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#read both files\n",
    "dF03= getATL03data(localFilePath, beamStr)    # georgeVI\n",
    "# dF03_e= getATL03data(localFilePath2, beamStr)  #edgeworth\n",
    "\n",
    "#plot\n",
    "\n",
    "coast=shapereader.natural_earth(resolution='10m',category='physical',name='coastline')\n",
    "\n",
    "coastlines=shapereader.Reader(coast).geometries()\n",
    "# Generate a shorted version for mapping purposes\n",
    "dF03short=dF03.iloc[::1000, :]\n",
    "dF03short.head(5)\n",
    "\n",
    "# dF03short_e=dF03_e.iloc[::1000, :]\n",
    "# dF03short_e.head(5)\n",
    "var='heights'\n",
    "\n",
    "%matplotlib widget\n",
    "ax=plt.figure(figsize=(8,8), dpi= 90)\n",
    "\n",
    "# Make a new \"NorthPolarStereo\" projection instance\n",
    "ax = plt.axes(projection=ccrs.SouthPolarStereo(true_scale_latitude=-70))\n",
    "plt.scatter(dF03short['lons'], dF03short['lats'],c=dF03short[var], cmap='viridis', transform=ccrs.PlateCarree())\n",
    "# plt.scatter(dF03short_e['lons'], dF03short_e['lats'],c=dF03short_e[var], cmap='viridis', transform=ccrs.PlateCarree())\n",
    "#ax.coastlines()\n",
    "ax.add_geometries(coastlines,ccrs.Geodetic(),edgecolor='k',facecolor='none')\n",
    "#ax.drawmeridians()\n",
    "plt.colorbar(label=var, shrink=0.5, extend='both')\n",
    "plt.title('Location of Granules Selected')\n",
    "# Limit the map to -60 degrees latitude and below.\n",
    "ax.set_extent([-75, -50, -70, -65], ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3 Beam plot  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "# data_dir='Outputs/'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "   sys.path.append(module_path)\n",
    "\n",
    "\n",
    "# make sure we're dealing with the most recent version of any code we're using\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ATL03_file=glob(dataDir + '*ATL03*.h5')\n",
    "ATL06_file=glob(dataDir + '*ATL06*.h5')\n",
    "# ATL03_file=glob('/home/jovyan/crackup/Outputs_selected/edgeworth/*ATL03*.h5')\n",
    "# ATL06_file=glob('/home/jovyan/crackup/Outputs_selected/edgeworth/*ATL06*.h5')\n",
    "\n",
    "%matplotlib widget\n",
    "beamNum = [1,3,5]\n",
    "ymin = [260,20,60]\n",
    "ymax = [350,200,90]\n",
    "\n",
    "f = h5py.File(ATL03_file[0], 'r')  # keep it open\n",
    "beam = [k for k in f.keys() if k.startswith('gt')]\n",
    "\n",
    "f6 = h5py.File(ATL06_file[0], 'r')\n",
    "beam6 = [k for k in f6.keys() if k.startswith('gt')]\n",
    "\n",
    "lookfor = ['delta_time','h_li','h_li_sigma','latitude','longitude','segment_id','sigma_geo_h']\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1, sharex=True,figsize=(10,8))\n",
    "data = {}\n",
    "data6 = {}\n",
    "\n",
    "for ii,p in enumerate(beamNum):\n",
    "\n",
    "# Extract data from ATL03\n",
    "   data[beam[p]] = {}\n",
    "   data[beam[p]]['heights'] = {}\n",
    "\n",
    "   for key,val in f[beam[p]]['heights'].items():\n",
    "       data[beam[p]]['heights'][key] = val[:]\n",
    "\n",
    "# Assign flag based on confidence level\n",
    "   #-- 0=Land; 1=Ocean; 2=SeaIce; 3=LandIce; 4=InlandWater\n",
    "   conf = data[beam[p]]['heights']['signal_conf_ph'][:,3] #choose column three for LandIce\n",
    "    #-- background and buffer photons\n",
    "   bg = np.nonzero((conf == 0) | (conf == 1))\n",
    "   lc = np.nonzero(conf == 2)\n",
    "   mc = np.nonzero(conf == 3)\n",
    "   hc = np.nonzero(conf == 4)\n",
    "\n",
    "# Extract data from ATL06\n",
    "\n",
    "   #identify bad values to filter-out   \n",
    "   data6[beam6[p]] = {}\n",
    "   data6[beam6[p]]['land_ice_segments'] = {}\n",
    "\n",
    "#    print('what',f6[beam6[p]]['land_ice_segments'].keys())\n",
    "   for key,val in f6[beam6[p]]['land_ice_segments'].items():\n",
    "       if key in lookfor:\n",
    "           data6[beam6[p]]['land_ice_segments'][key] = val[:]\n",
    "   bv = np.nonzero(data6[beam6[p]]['land_ice_segments']['h_li'] < 5000) #replace 5000 w/ real no-data value\n",
    "\n",
    "#  Shorten variable names\n",
    "   D3x = data[beam[p]]['heights']['lat_ph']\n",
    "   D3y = data[beam[p]]['heights']['h_ph']\n",
    "   D6x = data6[beam6[p]]['land_ice_segments']['latitude'][bv]\n",
    "   D6y = data6[beam6[p]]['land_ice_segments']['h_li'][bv]\n",
    "\n",
    "#    ax[ii].plot(data[beam[p]]['heights']['delta_time'][conf>=2],data[beam[p]]['heights']['h_ph'][conf>=2],'k.',markersize=0.1)\n",
    "#    ax[ii].plot(D3x[bg],D3y[bg],marker='.',lw=0,markersize=0.1,color='gray',label='Background')\n",
    "   ax[ii].plot(D3x[lc],D3y[lc],marker='.',lw=0,markersize=0.1,color='darkorange',label='Low Confidence')\n",
    "   ax[ii].plot(D3x[mc],D3y[mc],marker='.',lw=0,markersize=0.2,color='mediumseagreen',label='Medium Confidence')\n",
    "   ax[ii].plot(D3x[hc],D3y[hc],marker='.',lw=0,markersize=0.3,color='darkorchid',label='High Confidence')\n",
    "   ax[ii].plot(D6x,D6y,'r:',lw=1.5,label='ATL06 Surface')\n",
    "\n",
    "\n",
    "#  labels & limits\n",
    "   ax[ii].set_title('Beam ' + str(ii+1))\n",
    "   ax[ii].set_ylim(ymin[ii],ymax[ii])\n",
    "   ax[ii].set_xlim(-64.35,-64.32)\n",
    "   ax[ii].set_ylabel('Elevation (m)')\n",
    "    \n",
    "ax[ii].set_xlabel('Latitude (dec degrees)')\n",
    "lgd = ax[0].legend(loc=0,frameon=False)\n",
    "lgd.get_frame().set_alpha(1.0)\n",
    "for line in lgd.get_lines():\n",
    "    line.set_linewidth(6)\n",
    "plt.show()\n",
    "f.close()\n",
    "f6.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Moving stats on Elevation Differences </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import os\n",
    "import h5py\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "data_dir='Outputs/'\n",
    "from itertools import islice\n",
    "from scipy import interpolate\n",
    "import pyproj\n",
    "import os, csv\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "# make sure we're dealing with the most recent version of any code we're using\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create functions\n",
    "\n",
    "#by Suzanne Dickinson\n",
    "def moving_average(iterable, n):\n",
    "    # create an iterable object from input argument\n",
    "    it = iter(iterable)\n",
    "    piece = list(islice(it, n))\n",
    "    while piece:\n",
    "        yield np.mean(piece), np.median(piece), np.std(piece)\n",
    "        # yield gives back a generator, which needs to be iterated upon to get the info out.\n",
    "        piece = list(islice(it, n))\n",
    "        \n",
    "#from Fernando Paolo & Johan Nilsson's utilities\n",
    "def transform_coord(proj1, proj2, x, y):\n",
    "    \"\"\"Transform coordinates from proj1 to proj2 (EPSG num).\"\"\"\n",
    "    \n",
    "    # Set full EPSG projection strings\n",
    "    proj1 = pyproj.Proj(\"+init=EPSG:\"+proj1)\n",
    "    proj2 = pyproj.Proj(\"+init=EPSG:\"+proj2)\n",
    "    \n",
    "    # Convert coordinates\n",
    "    return pyproj.transform(proj1, proj2, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read the csv file necessary for looping\n",
    "# with open('region.csv') as csvfile:\n",
    "#     rows = csv.reader(csvfile)\n",
    "#     region = {row[0]:[row[1].strip(),row[2].strip()] for row in rows}   # region name : [datadir, shape filename]\n",
    "#     dival = {'dataDir':0,'shapefile':1}\n",
    "\n",
    "# reg = 'Edgeworth'\n",
    "# dataDir = region[reg][dival['dataDir']]\n",
    "# shapef = region[reg][dival['shapefile']]\n",
    "\n",
    "# print(dataDir,shapef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam gt1r\n",
      "698\n",
      "beam gt2r\n",
      "700\n",
      "beam gt3r\n",
      "742\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "#identify files\n",
    "ATL03_file=glob(dataDir + '*ATL03*.h5')\n",
    "ATL06_file=glob(dataDir + '*ATL06*.h5')\n",
    "beamNum = [1,3,5]\n",
    "\n",
    "for j,fi in enumerate(ATL03_file):\n",
    "    date, rgt, release, ver = fi.split('_')[2:]\n",
    "\n",
    "    for fi6 in ATL06_file:\n",
    "        date6, rgt6, release6, ver6 = fi6.split('_')[2:]\n",
    "        if date==date6 and rgt==rgt6 and release==release6 and ver==ver6:\n",
    "\n",
    "            #read data\n",
    "            f = h5py.File(ATL03_file[0], 'r')  # keep it open\n",
    "            beam = [k for k in f.keys() if k.startswith('gt')]\n",
    "            f6 = h5py.File(ATL06_file[0], 'r')\n",
    "            beam6 = [k for k in f6.keys() if k.startswith('gt')]\n",
    "\n",
    "            #set up dictionary\n",
    "            lookfor = ['delta_time','h_li','h_li_sigma','latitude','longitude','segment_id','sigma_geo_h']\n",
    "\n",
    "            #create base figures\n",
    "            fig1, ax1 = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True, figsize=(8,8))\n",
    "            fig2, ax2 = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True, figsize=(8,8))\n",
    "\n",
    "            #pull data and fill-in structures\n",
    "            data = {}\n",
    "            data6 = {}\n",
    "            window = 20 #specify moving window size\n",
    "            for ii,p in enumerate(beamNum):\n",
    "                print('beam',beam[p])\n",
    "                data[beam[p]] = {}\n",
    "                data[beam[p]]['heights'] = {}\n",
    "\n",
    "                for key,val in f[beam[p]]['heights'].items():\n",
    "                    data[beam[p]]['heights'][key] = val[:]\n",
    "\n",
    "                #-- 0=Land; 1=Ocean; 2=SeaIce; 3=LandIce; 4=InlandWater\n",
    "                conf = data[beam[p]]['heights']['signal_conf_ph'][:,3]\n",
    "                lat_ph = np.array(data[beam[p]]['heights']['lat_ph'][conf>=2])\n",
    "                lon_ph = np.array(data[beam[p]]['heights']['lon_ph'][conf>=2])\n",
    "                h_ph   = np.array(data[beam[p]]['heights']['h_ph'][conf>=2])\n",
    "                t_ph   = np.array(data[beam[p]]['heights']['delta_time'][conf>=2])\n",
    "\n",
    "                havg = np.array([])\n",
    "                hmed = np.array([])\n",
    "                hstd= np.array([])\n",
    "                for stats in moving_average(h_ph,window):\n",
    "                    havg = np.append(havg,stats[0])\n",
    "                    hmed = np.append(hmed,stats[1])\n",
    "                    hstd = np.append(hstd,stats[2])\n",
    "\n",
    "                lat_avg = np.array([])\n",
    "                for stats in moving_average(lat_ph,window):\n",
    "                    lat_avg = np.append(lat_avg,stats[0])\n",
    "                lon_avg = np.array([])\n",
    "                for stats in moving_average(lon_ph,window):\n",
    "                    lon_avg = np.append(lon_avg,stats[0])\n",
    "                x_avg, y_avg = transform_coord('4326', '3031', lon_avg, lat_avg)\n",
    "\n",
    "                data6[beam6[p]] = {}\n",
    "                data6[beam6[p]]['land_ice_segments'] = {}\n",
    "\n",
    "                for key,val in f6[beam6[p]]['land_ice_segments'].items():\n",
    "                    if key in lookfor:\n",
    "                        data6[beam6[p]]['land_ice_segments'][key] = val[:]\n",
    "                h_li = data6[beam6[p]]['land_ice_segments']['h_li']\n",
    "                h_li[h_li>3e38]= np.nan\n",
    "                lat_li = data6[beam6[p]]['land_ice_segments']['latitude']\n",
    "                lon_li = data6[beam6[p]]['land_ice_segments']['longitude']\n",
    "                print(len(lon_li))\n",
    "                x_li, y_li = transform_coord('4326', '3031', lon_li, lat_li)\n",
    "\n",
    "                # print(find_nearest(pointlat,photlat[200]))\n",
    "                elev_diff = np.zeros_like(y_avg)\n",
    "                for i,reflat in enumerate(y_avg):\n",
    "                    dist_array = np.sqrt((x_li-x_avg[i])**2 + (y_li-reflat)**2)\n",
    "                    idx = np.where(dist_array == dist_array.min())\n",
    "                #     idx = find_nearest(pointlat,reflat)\n",
    "                #     if np.abs(pointlat[idx]-photlat[i]) <= 20:\n",
    "                    if dist_array[idx] <= 20:\n",
    "                        elev_diff[i] = h_li[idx] - havg[i]\n",
    "                    else:\n",
    "                        elev_diff[i] = np.NaN\n",
    "                #         photlat[i] = 0\n",
    "\n",
    "                if ii<2:\n",
    "                    ax1[ii].plot(lat_ph,h_ph,'.',color=(0.3,0.3,0.3),markersize=0.7,label='ATL03')\n",
    "                    ax1[ii].plot(lat_avg,havg,'r',linewidth=0.8,label='moving average')\n",
    "                    ax1[ii].plot(lat_avg,hmed,'g',linewidth=0.6,label='moving median')\n",
    "                    ax1[ii].plot(lat_avg,havg+hstd,'c--',linewidth=0.3,label='moving stdev')\n",
    "                    ax1[ii].plot(lat_avg,havg-hstd,'c--',linewidth=0.3)\n",
    "                    ax1[ii].plot(lat_li,h_li,'k.',label='ATL06')\n",
    "                    ax1[ii].set_title(beam[p])\n",
    "            #         ax1[ii].set_ylim(0,80)\n",
    "            #         ax1[ii].set_xlim(-64.38,-64.37)\n",
    "\n",
    "                    ax2[ii].plot(lat_avg,elev_diff,'.',color=(0.3,0.3,0.3),markersize=0.7,label='ATL06 minus ATL03')\n",
    "                    ax2[ii].set_title(beam[p])\n",
    "            #         ax2[ii].set_ylim(-10,60)\n",
    "            #         ax2[ii].set_xlim(-64.38,-64.37)\n",
    "\n",
    "            lgd = ax1[0].legend(loc=0,frameon=True)\n",
    "            lgd.get_frame().set_alpha(1.0)\n",
    "            for line in lgd.get_lines():\n",
    "                line.set_linewidth(6)\n",
    "\n",
    "            lgd = ax2[0].legend(loc=2,frameon=True)\n",
    "            lgd.get_frame().set_alpha(1.0)\n",
    "            for line in lgd.get_lines():\n",
    "                line.set_linewidth(6)\n",
    "\n",
    "\n",
    "            plt.show()\n",
    "            f.close()\n",
    "            f6.close()\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics on elevation differences over the full domain\n",
      "Mean ATL06-ATL03 elevations =  0.12  m\n",
      "Median ATL06-ATL03 elevations =  0.08  m\n",
      "St. dev. in ATL06-ATL03 elevations =  1.32  m\n"
     ]
    }
   ],
   "source": [
    "#statistics over the full domain\n",
    "print('Statistics on elevation differences over the full domain')\n",
    "print('Mean ATL06-ATL03 elevations = ',np.around(np.nanmean(elev_diff),2),' m')\n",
    "print('Median ATL06-ATL03 elevations = ',np.around(np.nanmedian(elev_diff),2),' m')\n",
    "print('St. dev. in ATL06-ATL03 elevations = ',np.around(np.nanstd(elev_diff),2),' m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats over the profile zoom window\n",
      "Mean ATL06-ATL03 elevations =  0.1  m\n",
      "Median ATL06-ATL03 elevations =  0.12  m\n",
      "St. Dev. ATL06-ATL03 elevations =  1.56  m\n"
     ]
    }
   ],
   "source": [
    "#pull out limits of plot and display statistics over that region\n",
    "zoom_lims = ax1[0].get_xlim()\n",
    "# print(zoom_lims)\n",
    "ind_y = np.where((lat_avg>=zoom_lims[0]) & (lat_avg<=zoom_lims[1]))\n",
    "print('Stats over the profile zoom window')\n",
    "print('Mean ATL06-ATL03 elevations = ',np.around(np.nanmean(elev_diff[ind_y]),2),' m')\n",
    "print('Median ATL06-ATL03 elevations = ',np.around(np.nanmedian(elev_diff[ind_y]),2),' m')\n",
    "print('St. Dev. ATL06-ATL03 elevations = ',np.around(np.nanstd(elev_diff[ind_y]),2),' m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Plotting the tracks on a Landsat image </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "### Now to download it, open the terminal in jupyter labs and type:\n",
    "# need this to be real /home/jovyan/data/\n",
    "\n",
    "#make a data directory\n",
    "!mkdir -p /home/jovyan/data/\n",
    "#pull data\n",
    "!aws --no-sign-request s3 cp s3://pangeo-data-upload-oregon/icesat2/data-access-outputs/LC08_L1GT_217105_20171203_20171207_01_T2_B8.TIF /home/jovyan/data/ #pan\n",
    "!aws --no-sign-request s3 cp s3://pangeo-data-upload-oregon/icesat2/data-access-outputs/LC08_L1GT_217105_20171203_20171207_01_T2_B4.TIF /home/jovyan/data/ #red\n",
    "!aws --no-sign-request s3 cp s3://pangeo-data-upload-oregon/icesat2/data-access-outputs/LC08_L1GT_217105_20171203_20171207_01_T2_B3.TIF /home/jovyan/data/ #green\n",
    "!aws --no-sign-request s3 cp s3://pangeo-data-upload-oregon/icesat2/data-access-outputs/LC08_L1GT_217105_20171203_20171207_01_T2_B2.TIF /home/jovyan/data/ #blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# packages:\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import rasterio.mask\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from shapely.geometry import Point, Polygon\n",
    "fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n",
    "\n",
    "\n",
    "## Landsat:\n",
    "DATA_DIR = Path('/home/jovyan/data/')\n",
    "# LANDSAT_FILENAMES = [\n",
    "#     'LC08_L1GT_217105_20171203_20171207_01_T2_B4.TIF',\n",
    "#     'LC08_L1GT_217105_20171203_20171207_01_T2_B3.TIF',\n",
    "#     'LC08_L1GT_217105_20171203_20171207_01_T2_B2.TIF',\n",
    "# ]\n",
    "ROI_SHAPEFILE = shapef\n",
    "\n",
    "# define cartopy crs for the raster\n",
    "src_crs = ccrs.SouthPolarStereo(true_scale_latitude=-71)\n",
    "\n",
    "# load roi shapefile for plotting and clipping image\n",
    "gdf = gpd.read_file(ROI_SHAPEFILE).to_crs(src_crs.proj4_init)\n",
    "\n",
    "rgb = []\n",
    "for filename in LANDSAT_FILENAMES:\n",
    "    with rasterio.open(DATA_DIR / filename, 'r') as src:\n",
    "        # crop to our ROI\n",
    "        im, transform = rasterio.mask.mask(src, gdf.geometry, crop=True)\n",
    "\n",
    "        # move first axis to last\n",
    "        im = np.moveaxis(im, 0, -1)\n",
    "\n",
    "        # Set 0 values (no data) to nan\n",
    "        im = im.astype(float)\n",
    "        im[im == 0] = np.nan\n",
    "\n",
    "        rgb.append(im)\n",
    "\n",
    "# merge bands\n",
    "rgb = np.dstack(rgb)\n",
    "height, width, _ = rgb.shape\n",
    "\n",
    "# mask zero values (no data)\n",
    "mask = np.isnan(rgb)\n",
    "rgb = np.ma.masked_where(mask, rgb)\n",
    "\n",
    "# normalize\n",
    "rgb = rgb / rgb.max()\n",
    "\n",
    "# use mask as transparency array\n",
    "flat_mask = np.logical_not(np.expand_dims(mask.any(axis=-1), axis=-1))\n",
    "rgba = np.dstack([rgb, flat_mask])\n",
    "\n",
    "# calculate extent of raster\n",
    "xmin = transform[2]\n",
    "xmax = transform[2] + transform[0] * width\n",
    "ymin = transform[5] + transform[4] * height\n",
    "ymax = transform[5]\n",
    "\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=src_crs)\n",
    "\n",
    "# plot raster\n",
    "ax.imshow(\n",
    "    rgba,\n",
    "    origin='upper',\n",
    "    extent=[xmin, xmax, ymin, ymax],\n",
    "    transform=src_crs,\n",
    "    interpolation='nearest',\n",
    ")\n",
    "\n",
    "# plot coastlines\n",
    "#ax.coastlines(resolution='10m')\n",
    "\n",
    "xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "ax.set_xlim((xmin, xmax))\n",
    "ax.set_ylim((ymin, ymax))\n",
    "gdf.boundary.plot(ax=ax, color=None);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['atl06_quality_summary', 'bias_correction', 'delta_time', 'dem', 'fit_statistics', 'geophysical', 'ground_track', 'h_li', 'h_li_sigma', 'latitude', 'longitude', 'segment_id', 'sigma_geo_h']>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab the ICESat ATL06 file\n",
    "ATL06_file=glob(dataDir + '*ATL06*.h5')[0]\n",
    "f=h5py.File(ATL06_file, 'r')\n",
    "f['gt1r/land_ice_segments'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the data, read in 3 different beams\n",
    "with h5py.File(ATL06_file, 'r') as f:  # open file\n",
    "    x1 = f['gt1r/land_ice_segments/longitude'][:]                       # read data into memory\n",
    "    y1 = f['gt1r/land_ice_segments/latitude'][:]                           # get pointer to data on disk\n",
    "    z1 = f['gt1r/land_ice_segments/h_li'][:] \n",
    "    x2 = f['gt2r/land_ice_segments/longitude'][:]                      \n",
    "    y2 = f['gt2r/land_ice_segments/latitude'][:]\n",
    "    z2 = f['gt2r/land_ice_segments/h_li'][:] \n",
    "    x3 = f['gt3r/land_ice_segments/longitude'][:]                      \n",
    "    y3 = f['gt3r/land_ice_segments/latitude'][:]\n",
    "    z3 = f['gt3r/land_ice_segments/h_li'][:]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we concatenate the data (x,y,z) and add variable, b, in order to keep track of the beam. We then create a dataframe, df, using pandas.  Note that this is the entire graticule (?) and is not yet subset to our Edgeworth area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the data\n",
    "x=np.concatenate([x1,x2,x3],axis=0)\n",
    "y=np.concatenate([y1,y2,y3],axis=0)\n",
    "z=np.concatenate([z1,z2,z3],axis=0)\n",
    "b=np.zeros(x.shape)\n",
    "\n",
    "# you will be tempted to redefine these indices in the future\n",
    "# do not do it.  python indices are weird and bruce has confirmed that these are right\n",
    "\n",
    "ind1=len(x1)\n",
    "ind2=len(x1)+len(x2)\n",
    "ind3=len(x1)+len(x2)+len(x3)\n",
    "\n",
    "# create the beam index:\n",
    "b[0:ind1]=1\n",
    "b[ind1:ind2]=2\n",
    "b[ind2:ind3]=3\n",
    "\n",
    "df = pd.DataFrame({'lon':x,'lat':y,'z':z,'b':b})\n",
    "\n",
    "# get rid of no data values in z\n",
    "df['z'][df['z']==df['z'].max()]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>z</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2140.000000</td>\n",
       "      <td>2140.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-59.953262</td>\n",
       "      <td>-64.336801</td>\n",
       "      <td>343.808990</td>\n",
       "      <td>2.020561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.055517</td>\n",
       "      <td>0.039665</td>\n",
       "      <td>428.126038</td>\n",
       "      <td>0.820237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-60.038012</td>\n",
       "      <td>-64.411946</td>\n",
       "      <td>19.659355</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-60.012868</td>\n",
       "      <td>-64.370584</td>\n",
       "      <td>60.647812</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-59.952814</td>\n",
       "      <td>-64.337293</td>\n",
       "      <td>171.848389</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-59.895992</td>\n",
       "      <td>-64.303844</td>\n",
       "      <td>474.141724</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-59.869037</td>\n",
       "      <td>-64.257054</td>\n",
       "      <td>1849.275635</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lon          lat            z            b\n",
       "count  2140.000000  2140.000000  2017.000000  2140.000000\n",
       "mean    -59.953262   -64.336801   343.808990     2.020561\n",
       "std       0.055517     0.039665   428.126038     0.820237\n",
       "min     -60.038012   -64.411946    19.659355     1.000000\n",
       "25%     -60.012868   -64.370584    60.647812     1.000000\n",
       "50%     -59.952814   -64.337293   171.848389     2.000000\n",
       "75%     -59.895992   -64.303844   474.141724     3.000000\n",
       "max     -59.869037   -64.257054  1849.275635     3.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the stats\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a geopandas database\n",
    "\n",
    "#create a new geometry for the lat, lon - we will use this to cut data \n",
    "df['geometry'] = list(zip(df['lon'], df['lat']))\n",
    "df['geometry'] = df['geometry'].apply(Point)\n",
    "\n",
    "#create a geopandas database.\n",
    "gdf_outline = gpd.GeoDataFrame(df, crs={'init' :'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the shapefile\n",
    "kml_filepath=shapef\n",
    "\n",
    "#Return a GeoDataFrame object\n",
    "edw = gpd.read_file(kml_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the data\n",
    "icesat_in = gpd.sjoin(gdf_outline, edw, op='intersects', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<cartopy.mpl.geoaxes.GeoAxesSubplot at 0x7fc15dc4ca90>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%matplotlib widget\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(10,16))\n",
    "ax = plt.axes(projection=src_crs)\n",
    "\n",
    "# plot raster\n",
    "ax.imshow(\n",
    "    rgba,\n",
    "    origin='upper',\n",
    "    extent=[xmin, xmax, ymin, ymax],\n",
    "    transform=src_crs,\n",
    "    interpolation='nearest',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# plot coastlines\n",
    "#ax.coastlines(resolution='10m')\n",
    "\n",
    "xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "ax.set_xlim((xmin, xmax))\n",
    "ax.set_ylim((ymin, ymax))\n",
    "gdf.boundary.plot(ax=ax, color=None);\n",
    "gdf.boundary.plot(ax=ax, color=None);\n",
    "\n",
    "icesat_in.plot(ax=ax,transform=ccrs.PlateCarree(),markersize=1,column=icesat_in['z'],cmap='viridis',vmax=200, vmin=0,legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with panchromatic because it's easier to see crevasses   \n",
    "with rasterio.open('/home/jovyan/data/LC08_L1GT_217105_20171203_20171207_01_T2_B8.TIF', 'r') as src:\n",
    "        # crop to our ROI\n",
    "        im, transform = rasterio.mask.mask(src, gdf.geometry, crop=True)\n",
    "\n",
    "        # move first axis to last\n",
    "        im = np.moveaxis(im, 0, -1)\n",
    "\n",
    "        # Set 0 values (no data) to nan\n",
    "        im = im.astype(float)\n",
    "        im[im == 0] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# mask zero values (no data)\n",
    "mask = np.isnan(im)\n",
    "im = np.ma.masked_where(mask, im)\n",
    "\n",
    "# normalize\n",
    "im = im / im.max()\n",
    "\n",
    "# height + width of image\n",
    "height_p, width_p, _ = im.shape\n",
    "\n",
    "\n",
    "\n",
    "# calculate extent of raster\n",
    "xp_min = transform[2]\n",
    "xp_max = transform[2] + transform[0] * width_p\n",
    "yp_min = transform[5] + transform[4] * height_p\n",
    "yp_max = transform[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b07103407214a2ab104609155904e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<cartopy.mpl.geoaxes.GeoAxesSubplot at 0x7fc15d977e80>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure(figsize=(10, 16))\n",
    "ax = plt.axes(projection=src_crs)\n",
    "\n",
    "# plot raster\n",
    "ax.imshow(\n",
    "    im[:,:,0],\n",
    "    origin='upper',\n",
    "    extent=[xp_min, xp_max, yp_min, yp_max],\n",
    "    transform=src_crs,\n",
    "    interpolation='nearest',\n",
    "    cmap='gray'\n",
    ")\n",
    "\n",
    "# plot coastlines\n",
    "#ax.coastlines(resolution='10m')\n",
    "\n",
    "xp_min, yp_min, xp_max, yp_max = gdf.total_bounds\n",
    "ax.set_xlim((xp_min, xp_max))\n",
    "ax.set_ylim((yp_min, yp_max))\n",
    "gdf.boundary.plot(ax=ax, color=None);\n",
    "gdf.boundary.plot(ax=ax, color=None);\n",
    "\n",
    "icesat_in.plot(ax=ax,transform=ccrs.PlateCarree(),markersize=1,column=icesat_in['z'],cmap='viridis',vmax=200, vmin=0,legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
